{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"18oMhZsADOw9I04oHSL0zTn3KolMeg5oA","authorship_tag":"ABX9TyPBbJgbaCWVGIgmaPh0ks/3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Question 1**"],"metadata":{"id":"hQTJiuwnMjmt"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.datasets import USPS\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n","from torch.utils.tensorboard import SummaryWriter\n","from sklearn.metrics import precision_recall_curve, auc\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out\n","\n","# Load USPS dataset\n","transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=1),\n","    transforms.Resize((28, 28)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","train_dataset = USPS(root='./data', train=True, download=True, transform=transform)\n","test_dataset = USPS(root='./data', train=False, download=True, transform=transform)\n","\n","batch_size = 64\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","input_size = 28*28\n","hidden_size = 128\n","num_classes = 10\n","learning_rate = 0.001\n","num_epochs = 10\n","\n","\n","mlp_model = MLP(input_size, hidden_size, num_classes).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n","\n","# TensorBoard logging\n","writer = SummaryWriter()\n","\n","\n","for epoch in range(num_epochs):\n","    for images, labels in train_loader:\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","\n","\n","        outputs = mlp_model(images)\n","        loss = criterion(outputs, labels)\n","\n","\n","        mlp_optimizer.zero_grad()\n","        loss.backward()\n","        mlp_optimizer.step()\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","\n","mlp_model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    all_preds = []\n","    all_labels = []\n","    for images, labels in test_loader:\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","        outputs = mlp_model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        all_preds.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","        precision, recall, _ = precision_recall_curve(all_labels, all_preds, pos_label=1)\n","        pr_auc = auc(recall, precision)\n","\n","        # Log precision-recall curve\n","        writer.add_pr_curve('Precision-Recall Curve', torch.tensor(all_labels), torch.tensor(all_preds), epoch)\n","\n","        # Log PR AUC\n","        writer.add_scalar('PR AUC', pr_auc, epoch)\n","mlp_accuracy = correct / total\n","mlp_precision = precision_score(all_labels, all_preds, average='macro')\n","mlp_recall = recall_score(all_labels, all_preds, average='macro')\n","mlp_confusion_matrix = confusion_matrix(all_labels, all_preds)\n","\n","print('MLP Accuracy: {:.4f}'.format(mlp_accuracy))\n","print('MLP Precision: {:.4f}'.format(mlp_precision))\n","print('MLP Recall: {:.4f}'.format(mlp_recall))\n","print('MLP Confusion Matrix:')\n","print(mlp_confusion_matrix)\n","\n","writer.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blR8Q0fNDxEp","executionInfo":{"status":"ok","timestamp":1710675288838,"user_tz":-330,"elapsed":23935,"user":{"displayName":"Harshitha Nagella (MT19AIE243)","userId":"03644837138177246466"}},"outputId":"efe0cae2-fb3c-4116-8de8-f7ed107e6229"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 0.2628\n","Epoch [2/10], Loss: 0.1188\n","Epoch [3/10], Loss: 0.2470\n","Epoch [4/10], Loss: 0.1764\n","Epoch [5/10], Loss: 0.1250\n","Epoch [6/10], Loss: 0.0496\n","Epoch [7/10], Loss: 0.0831\n","Epoch [8/10], Loss: 0.0356\n","Epoch [9/10], Loss: 0.0791\n","Epoch [10/10], Loss: 0.0723\n","MLP Accuracy: 0.9402\n","MLP Precision: 0.9385\n","MLP Recall: 0.9329\n","MLP Confusion Matrix:\n","[[354   0   2   0   1   0   1   0   1   0]\n"," [  0 253   1   1   3   0   4   0   1   1]\n"," [  2   0 188   2   1   2   1   0   2   0]\n"," [  3   0   3 142   0  12   0   0   5   1]\n"," [  0   2   4   0 187   1   1   0   0   5]\n"," [  3   0   0   4   2 149   0   0   0   2]\n"," [  1   0   3   0   2   0 164   0   0   0]\n"," [  1   0   5   1   6   0   0 130   2   2]\n"," [  5   0   3   2   1   2   0   0 150   3]\n"," [  0   1   1   0   2   2   0   0   1 170]]\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import USPS\n","import torchvision.transforms as transforms\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n","from torch.utils.tensorboard import SummaryWriter\n","\n","\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc1 = nn.Linear(32 * 4 * 4, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = x.view(-1, 32 * 4 * 4)  # Flatten the feature maps\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","train_dataset = USPS(root='./data', train=True, download=True, transform=transform)\n","test_dataset = USPS(root='./data', train=False, download=True, transform=transform)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n","\n","\n","cnn_model = CNN()\n","\n","criterion = nn.CrossEntropyLoss()\n","cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n","\n","\n","writer = SummaryWriter()\n","\n","\n","cnn_model.train()\n","for epoch in range(5):  # Train for 5 epochs\n","    for images, labels in train_loader:\n","        cnn_optimizer.zero_grad()\n","        outputs = cnn_model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        cnn_optimizer.step()\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","cnn_model.eval()\n","cnn_predictions = []\n","cnn_true_labels = []\n","for images, labels in test_loader:\n","    outputs = cnn_model(images)\n","    _, predicted = torch.max(outputs.data, 1)\n","    cnn_predictions.extend(predicted.numpy())\n","    cnn_true_labels.extend(labels.numpy())\n","\n","cnn_accuracy = accuracy_score(cnn_true_labels, cnn_predictions)\n","print(\"CNN Accuracy:\", cnn_accuracy)\n","\n","cnn_precision = precision_score(cnn_true_labels, cnn_predictions, average='macro')\n","cnn_recall = recall_score(cnn_true_labels, cnn_predictions, average='macro')\n","cnn_confusion_matrix = confusion_matrix(cnn_true_labels, cnn_predictions)\n","\n","print('CNN Accuracy: {:.4f}'.format(cnn_accuracy))\n","print('CNN Precision: {:.4f}'.format(cnn_precision))\n","print('CNN Recall: {:.4f}'.format(cnn_recall))\n","print('CNN Confusion Matrix:')\n","print(cnn_confusion_matrix)\n","\n","writer.close()"],"metadata":{"id":"aXNFsAMpGrvS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710675063906,"user_tz":-330,"elapsed":21996,"user":{"displayName":"Harshitha Nagella (MT19AIE243)","userId":"03644837138177246466"}},"outputId":"940aaee7-bfc7-4c76-d88c-1c89308be57e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 0.2256\n","Epoch [2/10], Loss: 0.1768\n","Epoch [3/10], Loss: 0.1029\n","Epoch [4/10], Loss: 0.1881\n","Epoch [5/10], Loss: 0.0897\n","CNN Accuracy: 0.9412057797708022\n","CNN Accuracy: 0.9412\n","CNN Precision: 0.9373\n","CNN Recall: 0.9367\n","CNN Confusion Matrix:\n","[[347   0   1   0   2   3   4   1   0   1]\n"," [  0 256   0   0   3   0   3   2   0   0]\n"," [  2   0 185   5   2   2   0   2   0   0]\n"," [  0   0   3 146   0  16   0   0   0   1]\n"," [  0   2   2   0 188   0   2   2   0   4]\n"," [  1   0   0   3   0 154   0   0   0   2]\n"," [  0   0   2   0   3   2 163   0   0   0]\n"," [  0   0   1   1   4   0   0 140   0   1]\n"," [  5   1   1   4   1   9   0   1 139   5]\n"," [  0   1   0   0   1   1   0   3   0 171]]\n"]}]},{"cell_type":"markdown","source":["# **Question 2**"],"metadata":{"id":"AeVOqcizMtuF"}},{"cell_type":"code","source":["import torchvision.datasets\n","import torch.utils.data as data\n"],"metadata":{"id":"KojzOBH7SJvc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"I8tIKlUZU61U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torchvision.datasets import ImageNet\n","\n","\n","model = models.resnet101(pretrained=True)\n","\n","\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, num_classes)\n","\n","\n","transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","from torchvision.datasets.imagenet import parse_devkit_archive, parse_train_archive, parse_val_archive\n","\n","\n","train_dataset = ImageNet(root='/content/drive/MyDrive/Dlops', transform=transform, train=True)\n","test_dataset = ImageNet(root='/content/drive/MyDrive/Dlops', transform=transform, train=False)\n","\n","batch_size = 64\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","optimizers = [\n","    optim.Adam(model.parameters()),\n","    optim.Adagrad(model.parameters()),\n","    optim.RMSprop(model.parameters())\n","]\n","\n","\n","num_epochs = 10\n","for optimizer in optimizers:\n","    optimizer_name = optimizer.__class__.__name__\n","    print(f\"Training with {optimizer_name}\")\n","\n","    train_losses = []\n","    train_accuracies = []\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.cuda(), labels.cuda()\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            running_loss += loss.item()\n","\n","        epoch_loss = running_loss / len(train_loader)\n","        epoch_accuracy = correct / total\n","\n","        train_losses.append(epoch_loss)\n","        train_accuracies.append(epoch_accuracy)\n","\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"hX4KVA4sW5RY","executionInfo":{"status":"error","timestamp":1710678535145,"user_tz":-330,"elapsed":1673,"user":{"displayName":"Harshitha Nagella (MT19AIE243)","userId":"03644837138177246466"}},"outputId":"9200daac-4aa8-4c8d-9c7a-44a28b69b48d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"The archive ILSVRC2012_img_train.tar is not present in the root directory or is corrupted. You need to download it externally and place it in /content/drive/MyDrive/Dlops.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-8f597f1b8710>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Dlops'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Dlops'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/imagenet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverify_str_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_archives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mwnid_to_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_meta_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/imagenet.py\u001b[0m in \u001b[0;36mparse_archives\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mparse_train_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mparse_val_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/imagenet.py\u001b[0m in \u001b[0;36mparse_train_archive\u001b[0;34m(root, file, folder)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mmd5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marchive_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0m_verify_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mtrain_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/imagenet.py\u001b[0m in \u001b[0;36m_verify_archive\u001b[0;34m(root, file, md5)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;34m\"You need to download it externally and place it in {}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         )\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The archive ILSVRC2012_img_train.tar is not present in the root directory or is corrupted. You need to download it externally and place it in /content/drive/MyDrive/Dlops."]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import USPS\n","import torchvision.transforms as transforms\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# Define the CNN model\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc1 = nn.Linear(32 * 4 * 4, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = x.view(-1, 32 * 4 * 4)  # Flatten the feature maps\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","train_dataset = USPS(root='./data', train=True, download=True, transform=transform)\n","test_dataset = USPS(root='./data', train=False, download=True, transform=transform)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n","\n","cnn_model = CNN()\n","\n","criterion = nn.CrossEntropyLoss()\n","cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n","\n","writer = SummaryWriter()\n","\n","cnn_model.train()\n","num_epochs = 5\n","for epoch in range(num_epochs):  # Train for 5 epochs\n","    for images, labels in train_loader:\n","        cnn_optimizer.zero_grad()\n","        outputs = cnn_model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        cnn_optimizer.step()\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","# Save the trained model\n","torch.save(cnn_model.state_dict(), 'cnn_model.pth')\n","print(\"CNN model saved successfully\")\n","\n","cnn_model.eval()\n","cnn_predictions = []\n","cnn_true_labels = []\n","for images, labels in test_loader:\n","    outputs = cnn_model(images)\n","    _, predicted = torch.max(outputs.data, 1)\n","    cnn_predictions.extend(predicted.numpy())\n","    cnn_true_labels.extend(labels.numpy())\n","\n","cnn_accuracy = accuracy_score(cnn_true_labels, cnn_predictions)\n","print(\"CNN Accuracy:\", cnn_accuracy)\n","\n","cnn_precision = precision_score(cnn_true_labels, cnn_predictions, average='macro')\n","cnn_recall = recall_score(cnn_true_labels, cnn_predictions, average='macro')\n","cnn_confusion_matrix = confusion_matrix(cnn_true_labels, cnn_predictions)\n","\n","print('CNN Accuracy: {:.4f}'.format(cnn_accuracy))\n","print('CNN Precision: {:.4f}'.format(cnn_precision))\n","print('CNN Recall: {:.4f}'.format(cnn_recall))\n","print('CNN Confusion Matrix:')\n","print(cnn_confusion_matrix)\n","\n","writer.close()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2W27INg6q-L8","executionInfo":{"status":"ok","timestamp":1712210114900,"user_tz":-330,"elapsed":39203,"user":{"displayName":"Harshitha Nagella (MT19AIE243)","userId":"03644837138177246466"}},"outputId":"08e022fd-cc2b-4d33-af8a-a81513e3e44e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2 to ./data/usps.bz2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6579383/6579383 [00:01<00:00, 4545258.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Downloading https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2 to ./data/usps.t.bz2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1831726/1831726 [00:01<00:00, 1619517.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/5], Loss: 0.4830\n","Epoch [2/5], Loss: 0.1224\n","Epoch [3/5], Loss: 0.1047\n","Epoch [4/5], Loss: 0.2303\n","Epoch [5/5], Loss: 0.0245\n","CNN model saved successfully\n","CNN Accuracy: 0.931738913801694\n","CNN Accuracy: 0.9317\n","CNN Precision: 0.9268\n","CNN Recall: 0.9271\n","CNN Confusion Matrix:\n","[[350   0   1   1   2   0   4   0   0   1]\n"," [  0 252   1   1   1   0   6   2   0   1]\n"," [  1   0 186   5   0   1   0   1   3   1]\n"," [  0   0   3 156   0   6   0   0   0   1]\n"," [  0   1   3   0 163   3   2   0   2  26]\n"," [  1   0   0  10   0 147   0   0   0   2]\n"," [  0   0   2   0   2   4 160   0   2   0]\n"," [  0   0   1   3   3   0   0 135   0   5]\n"," [  4   0   0   7   0   2   0   2 148   3]\n"," [  0   0   0   1   0   0   0   2   1 173]]\n"]}]},{"cell_type":"code","source":["ls\n"],"metadata":{"id":"NEY-5zD85tTA","executionInfo":{"status":"ok","timestamp":1712213912481,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harshitha Nagella (MT19AIE243)","userId":"03644837138177246466"}},"outputId":"9681c561-64a5-4ae3-f244-157fc475d2c7","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}]}]}